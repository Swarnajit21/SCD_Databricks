{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57424825-61af-458f-829d-4a8893ab3755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.scd_implementation_source\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.scd_implementation_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202cb1fc-2af6-4043-9a46-b1ac3c9ca8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_data = spark.sql(\"select * from samples.tpch.customer\")\n",
    "customer_data.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"workspace.scd_implementation_source.customer_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa1d3b6-0568-4cc0-833f-6d8deb69d820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = spark.read.table(\"workspace.scd_implementation_source.customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80781fc9-11a6-4992-93ee-a40978d99c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load Data From Source and concatenate all columns into 'ConCatValue'\n",
    "source = source.withColumn(\"ConCatValue\", F.concat_ws(\"\", *source.columns))\n",
    "\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bb449f-6908-4d02-b5b0-15e28988a84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add IndCurrent, CreatedDate, and ModifiedDate columns\n",
    "source = source.withColumn(\"IndCurrent\", F.lit(1)).withColumn(\n",
    "    \"CreatedDate\", F.current_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba97830d-d132-4c24-8ec5-5aa06c095e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE workspace.scd_implementation_source.customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7973b128-e56e-418e-bc3a-fb17909f8445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "source = source.withColumn(\"storage_id\", F.row_number().over(window_spec))\n",
    "\n",
    "first_cols = [\"storage_id\"]\n",
    "other_cols = [col for col in source.columns if col not in first_cols]\n",
    "source = source.select(first_cols + other_cols)\n",
    "\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78369ef-8571-4866-95bf-9c8985dfe43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate SHA-256 hash of concatenated column values and drop 'ConCatValue'\n",
    "source = source.withColumn(\"RowHash\", F.sha2(F.col(\"ConCatValue\"), 256)).drop(\n",
    "    \"ConCatValue\"\n",
    ")\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b62f9d-ac4c-4d26-bb8e-701edfd342ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "source.write.mode(\"append\").saveAsTable(\n",
    "    \"workspace.scd_implementation_target.customer_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9ac7a4-73d7-483a-bb23-3206d2f4a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf = spark.read.table(\"workspace.scd_implementation_source.customer_data\")\n",
    "TargetDf = spark.read.table(\"workspace.scd_implementation_target.customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672469f3-9574-42a1-b3de-8dc6e76969ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to show only rows where 'franchiseID' is '3000001'\n",
    "# Display the filtered DataFrame for inspection\n",
    "SourceDf.filter(col(\"c_custkey\") == \"412450\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300b372d-4450-4689-8ad8-defe1f347218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Update the 'city' column in SourceDf:\n",
    "# For rows where 'franchiseID' equals '3000001', set the 'city' value to 'Tokyo Modified'.\n",
    "# For all other rows, retain the original 'city' value.\n",
    "SourceDf = SourceDf.withColumn(\n",
    "    \"customer_priority\",\n",
    "    when(col(\"c_mktsegment\") == \"BUILDING\", \"Priority Customer\").otherwise(\n",
    "        \"Not Priority\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display rows where 'franchiseID' is '3000001' to verify the 'city' column update.\n",
    "SourceDf.filter(col(\"customer_priority\") == \"Priority Customer\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f057b8fa-23ff-47e6-bf0c-a89304d1359d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "SourceDf = SourceDf.withColumn(\"RowHash\", F.concat_ws(\"\", *SourceDf.columns))\n",
    "SourceDf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9510aa20-4b38-48f8-a3f2-4a6a0c3b6ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf = (\n",
    "    SourceDf.withColumn(\"IndCurrent\", F.lit(1))\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp())\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())\n",
    ")\n",
    "SourceDf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed80e17a-df9c-44c1-936d-c7de54a78586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf = SourceDf.drop(\"customer_priority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b93f5d6-9ba4-4822-8ea3-4155eee397ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<delta.connect.tables.DeltaMergeBuilder at 0xffae64fdd790>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Static configuration\n",
    "table_name = \"workspace.scd_implementation_target.customer_data\"\n",
    "key_column = \"c_custkey\"\n",
    "timestamp_column = \"ModifiedDate\"\n",
    "hash_column = \"RowHash\"\n",
    "created_column = \"CreatedDate\"\n",
    "\n",
    "# Reference Delta table\n",
    "target_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Aliases\n",
    "src = SourceDf.alias(\"src\")\n",
    "tgt = target_table.alias(\"tgt\")\n",
    "\n",
    "# Columns to update (exclude key, timestamp, and created date)\n",
    "columns_to_update = [\n",
    "    col_name\n",
    "    for col_name in SourceDf.columns\n",
    "    if col_name not in [key_column, timestamp_column, created_column]\n",
    "]\n",
    "\n",
    "# Construct SET dictionary for update\n",
    "set_dict = {col_name: col(f\"src.{col_name}\") for col_name in columns_to_update}\n",
    "set_dict[timestamp_column] = current_timestamp()  # Add ModifiedDate explicitly\n",
    "\n",
    "# Perform SCD Type 1 MERGE\n",
    "tgt.merge(src, f\"tgt.{key_column} = src.{key_column}\").whenMatchedUpdate(\n",
    "    condition=col(f\"src.{hash_column}\") != col(f\"tgt.{hash_column}\"), set=set_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81c4884-6d5e-4164-9ac4-1655ed97fe39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceTable='workspace.scd_implementation_source.customer_data'\n",
    "TargetTable='workspace.scd_implementation_target.customer_data'\n",
    "SourceDf = spark.read.table(\"workspace.scd_implementation_source.customer_data\")\n",
    "TargetDf = spark.read.table(\"workspace.scd_implementation_target.customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f1492d-b343-44d9-a86f-06129113a451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#join with Target Table and create Flag\n",
    "TargetDf=spark.read.table(TargetTable).select(['c_custkey','RowHash','storage_id']).withColumnRenamed('RowHash','TargetHash')\n",
    "SourceDf=SourceDf.join(TargetDf, on =['c_custkey'], how='left').withColumn('Flag', F.when(col('TargetHash').isNull() | (col('TargetHash') != col('RowHash')), 'New').when(col('TargetHash') == col('RowHash'), 'NoChange').otherwise('Update'))\n",
    "# Drop the TargetHash column\n",
    "SourceDf=SourceDf.drop('TargetHash')\n",
    "SourceDf=SourceDf.filter(col(\"Flag\") == \"New\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c0f4ae-f0ac-4a28-9c7e-1c964f2cd15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<delta.connect.tables.DeltaMergeBuilder at 0xffae642f9610>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "table_name = \"workspace.scd_implementation_target.customer_data\"\n",
    "key_column = \"c_custkey\"\n",
    "hash_column = \"RowHash\"\n",
    "is_current_column = \"IndCurrent\"\n",
    "surrogate_key_column = \"storage_id\"\n",
    "created_column = \"CreatedDate\"\n",
    "\n",
    "# Reference Delta table\n",
    "target_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Add new columns to source DataFrame\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "SourceDf = SourceDf \\\n",
    "    .withColumn(surrogate_key_column, uuid_udf()) \\\n",
    "    .withColumn(created_column, current_timestamp()) \\\n",
    "    .withColumn(is_current_column, lit(1))\n",
    "\n",
    "# Use aliases properly\n",
    "src = SourceDf.alias(\"src\")\n",
    "tgt = target_table.alias(\"tgt\")\n",
    "\n",
    "# Use column expressions (not strings) in merge condition\n",
    "tgt.merge(\n",
    "    source=src,\n",
    "    condition=(\n",
    "        (col(f\"tgt.{key_column}\") == col(f\"src.{key_column}\")) &\n",
    "        (col(f\"tgt.{is_current_column}\") == lit(1))\n",
    "    )\n",
    ").whenMatchedUpdate(\n",
    "    condition=col(f\"tgt.{hash_column}\") != col(f\"src.{hash_column}\"),\n",
    "    set={\n",
    "        is_current_column: lit(0)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dffee00-30ee-435b-a677-95038cb0f576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7899010656982877,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Implementation of SCD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}